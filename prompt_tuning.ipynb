{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "from torch import nn\n",
    "from clip.model import CLIP\n",
    "from clip.simple_tokenizer import SimpleTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model: CLIP):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, soft_prompt_emb, prompt_token_ids):\n",
    "        x = soft_prompt_emb + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # shape: [num_classes, num_tokens, dim] -> [num_tokens, num_classes, dim]\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # shape: [num_tokens, num_classes, dim] -> [num_classes, num_tokens, dim]\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), prompt_token_ids.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftPrompt(nn.Module):\n",
    "    def __init__(self, class_names, clip_model: CLIP, tokenizer_fn, nctx=16) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.nctx = nctx\n",
    "        dtype = clip_model.dtype\n",
    "        dim, = clip_model.ln_final.weight.shape\n",
    "        self.ctx = nn.Parameter(torch.normal(mean=0, std=0.02, size=(nctx, dim), dtype=dtype))\n",
    "\n",
    "        self.num_classes = len(class_names)\n",
    "        dummy_texts = \" \".join(['X'] * nctx)\n",
    "\n",
    "        class_names = [name.replace(\"_\", \" \") for name in class_names]\n",
    "        prompt_texts = [f'{dummy_texts} {name}.' for name in class_names]\n",
    "\n",
    "        self.prompt_token_ids = torch.cat([tokenizer_fn(p) for p in prompt_texts])\n",
    "        with torch.no_grad():\n",
    "            embeddings = clip_model.token_embedding(self.prompt_token_ids).to(dtype)\n",
    "\n",
    "        self.register_buffer(\"prefix_emb\", embeddings[:, :1, :])  # SOT, shape: [num_classes, 1, dim]\n",
    "        self.register_buffer(\"suffix_emb\", embeddings[:, 1+nctx:, :])  # Class tokens and EOT, shape: [num_classes, *, dim]\n",
    "    \n",
    "    def forward(self):\n",
    "        ctx = self.ctx\n",
    "        ctx = ctx.unsqueeze(0).expand(self.num_classes, -1, -1)  # shape: [num_classes, nctx, dim]\n",
    "        soft_prompts = torch.cat([self.prefix_emb, ctx, self.suffix_emb], dim=1)\n",
    "        return soft_prompts, self.prompt_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPWithSoftPrompt(nn.Module):\n",
    "    def __init__(self, class_names, clip_model: CLIP, clip_tokenize_fn):\n",
    "        super().__init__()\n",
    "        self.create_soft_prompt = SoftPrompt(class_names, clip_model, clip_tokenize_fn)\n",
    "        self.visual_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, image):\n",
    "        image_features = self.visual_encoder(image.to(self.dtype))\n",
    "\n",
    "        soft_prompt_emb, prompt_token_ids = self.create_soft_prompt()\n",
    "        text_features = self.text_encoder(soft_prompt_emb, prompt_token_ids)\n",
    "\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits = logit_scale * image_features @ text_features.t()\n",
    "\n",
    "        return logits, image_features, text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, clip_preprocess = clip.load('RN50')\n",
    "class_names = ['elephant', 'person', 'fish', 'bird']\n",
    "model = CLIPWithSoftPrompt(class_names, clip_model, clip.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((16, 3, 224, 224))\n",
    "logts, im_feats, text_feats = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1024])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
